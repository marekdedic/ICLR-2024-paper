\section{Detailed experiment settings and results}\label{sec:app:experiment-details}

The proposed methods were experimentally verified on 10 publicly available datasets. The datasets Cora and CiteSeer \cite{yang_revisiting_2016} were used with the \enquote{full} train-test split as in \cite{chen_fastgcn_2018}. In addition, 2 variants of the Twitch dataset \cite{rozemberczki_multi-scale_2021} with the hignest node count (DE and EN) were used. Five medium sized datasets were also used, the PubMed dataset \cite{yang_revisiting_2016}, the DBLP dataset \cite{bojchevski_deep_2018}, the IMDB dataset \cite{fu_magnn_2020} and both variants of the Coauthor dataset \cite{shchur_pitfalls_2019}. Finally, one large dataset was used, the OGB ArXiv dataset \cite{hu_open_2021}.

The hyper-parameters for both the node2vec model used for the embedding training and the multi-layer perceptron used for downstream classification were initially set to values used in prior art (see \cite{hu_open_2021,fey_fast_2019}) and then manually fine-tuned for each dataset.

The achitecture of the algorithm was identical accross the dataset, with the only difference being in the values of the hyper-parameters, as listed in Table~\ref{tab:hyperparameter-values}. For the Cora dataset, the node2vec model generated an embedding in \( \mathfield{R}^{128} \) from \( 4 \) random walks of length \( 20 \) for each node with a context window of size \( 5 \). The optimizer ADAM \cite{kingma_adam:_2017} was used with a learning rate of \( 0.01 \) and batches of \( 128 \) samples. The model was trained for \( 5 \) epochs and in each step of the adaptive prolongation, \( 100 \) nodes were prolonged, until reaching the original graph (the value of \( n_p \) was calculated so that the total number of training epochs would match baseline model training). The MLP classifier using the embeddings featured \( 3 \) linear layers of \( 128 \) neurons with batch normalization after each layer. Each layer was normalized using dropout \cite{srivastava_dropout_2014} with the rate of \( 0.5 \). Finally, a linear layer was used for the class prediction. For the classifier, ADAM with a learning rate of \( 0.01 \) was used for \( 30 \) epochs of training with the cross-entropy loss function. Dataset features weren't used for the classifier training as the aim of this work is to compare the embeddings. The experiment was run \( 10 \) times end-to-end and results averaged. The experiments were implemented using PyTorch \cite{paszke_pytorch_2019} and PyTorch Geometric \cite{fey_fast_2019}.

\begin{table}
  \caption{Hyper-parameter values used for different datasets}
  \label{tab:hyperparameter-values}
  \centering
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{lrrrrrrrr}
      \toprule
      \textbf{Hyper-parameter} & \textbf{Cora} & \textbf{CiteSeer} & \textbf{PubMed} & \textbf{DBLP} & \textbf{Twitch} & \textbf{IMDB} & \textbf{ArXiv} & \textbf{Coauthor} \\
      \midrule
      Embedding dimension      & 128           & 32                & 64              & 32            & 128             & 128           & 128            & 128               \\
      \# of random walks       & 4             & 5                 & 3               & 2             & 10              & 40            & 10             & 40                \\
      Random walk length       & 20            & 20                & 40              & 20            & 80              & 100           & 80             & 10                \\
      Context window size      & 5             & 5                 & 20              & 5             & 3               & 5             & 20             & 5                 \\
      Node2vec learning rate   & 0.01          & 0.01              & 0.01            & 0.01          & 0.025           & 0.01          & 0.01           & 0.01              \\
      Node2vec batch size      & 128           & 128               & 128             & 128           & 128             & 256           & 128            & 256               \\
      Node2vec epochs          & 5             & 7                 & 1               & 1             & 5               & 1             & 1              & 1                 \\
      %\# of prolonged nodes    & 100           & 150               & 1000            & 800           & 200             & 1400          & 8000           & 1250              \\
      \# of MLP layers         & 3             & 3                 & 1               & 3             & 2               & 2             & 3              & 2                 \\
      MLP hidden layer width   & 128           & 256               & 128             & 256           & 64              & 64            & 256            & 16                \\
      Dropout rate             & 0.5           & 0.5               & 0.5             & 0.5           & 0.5             & 0.5           & 0.5            & 0.5               \\
      MLP learning rate        & 0.01          & 0.01              & 0.01            & 0.01          & 0.01            & 0.01          & 0.01           & 0.01              \\
      MLP epochs               & 30            & 80                & 300             & 300           & 500             & 100           & 300            & 100               \\
      \bottomrule
    \end{tabular}%
  }
\end{table}
