\section{Introduction}\label{sec:introduction}
Machine learning on graphs has, in recent years, seen an explosion in popularity, the underlying graph topology has, however, received much less attention. The main aim of this work is to explore the performance-complexity characteristics in the context of graph learning, as introduced in \cite{prochazka_downstream_2022}, mainly to enable learning on graphs that would otherwise be prohibitively large. Consider an undirected graph \( G \). The result of a repeated application of graph coarsening is a sequence of graphs \( G_0, G_1, G_2, \dots, G_L \) where \( G_0 = G \). Given a model \( M \) that operates on graphs, a performance metric, and a complexity metric, the sequence \( G_0, G_1, \dots, G_L \) corresponds to points in the performance-complexity plane, where advancing along the sequence generally hurts performance and decreases complexity. This performance-complexity characteristic allows for a choice of a \textbf{working point} tailored to the specific scenario. The method proposed in the rest of this work evaluates the graphs in reverse order, i.e. starting with the simplest one. As such, the algorithm only trains the model on a subset of simpler graphs, lowering the complexity of selecting the working point.
