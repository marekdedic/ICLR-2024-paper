\section{A step of adaptive prolongation}\label{sec:app:prolongation-step}

The sequence \( G_0, G_1, G_2, \dots, G_L \) as defined in Section \ref{sec:introduction} is generated in HARP consecutively. Let \( \varphi_i \) denote this mapping \( G_i = \varphi_i \left( G_{i - 1} \right) \). Following \cite{schulz_mining_2019}, we restrict the definition of such a coarsening \( \varphi_i \) to only consist of a series of edge contractions \( \mathcal{C} \subseteq E \left( G \right) \). Let us denote \( \Psi_K, \dots, \Psi_0 \) the embedding sequence produced by the adaptive prologation schema. A description of a single prolongation step from \( \Psi_{i + 1} \) to \( \Psi_i \) follows and is described in further detail in Algorithm \ref{fig:adaptive-prolongation}.

\begin{algorithm*}
  \caption{Adaptive prolongation}
  \label{alg:adaptive-prolongation}
  \begin{algorithmic}
    \Require $ G_0 $ \Comment The original graph
    \Require $ \bm{y}_\mathrm{train} $ \Comment Training labels
    \Require $ n_p $ \Comment The number of nodes to prolong
    \Require $ \Psi_{i + 1} $ \Comment The previous embedding
    \Require $ \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} $ \Comment A list of all the contractions yet to be reversed
    \Ensure $ \Psi_i $ \Comment The next embedding
    \Ensure $ \mathcal{C}_L^{(i)}, \dots, \mathcal{C}_0^{(i)} $ \Comment Updated contraction list without the prolonged contractions
    \Statex
    \State $ node\_order \gets \Call{get\_node\_order}{G_0, \Psi_{i+1}, \bm{y}_\mathrm{train}, \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)}} $
    \State $ \mathcal{C}_\mathrm{prolong} \gets \Call{select\_contractions}{node\_order, n_p, \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)}} $
    \State $ \Psi_i \gets $ use $ \mathcal{C}_\mathrm{prolong} $ to prolong the embedding $ \Psi_{i + 1} $
    \State $ \mathcal{C}_L^{(i)}, \dots, \mathcal{C}_0^{(i)} \gets $ remove contractions in $ \mathcal{C}_\mathrm{prolong} $ from $ \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} $
    \Statex
    \Function{get\_node\_order}{$ G_0, \Psi_{i+1}, \bm{y}_\mathrm{train}, \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} $}
        \State $ \Psi_0^\mathrm{temp} \gets $ use $ \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} $ to fully prolong the current embedding $ \Psi_{i+1} $ to $ G_0 $
        \State $ model \gets \Call{train\_downstream\_model}{\Psi_0^\mathrm{temp}, \bm{y}_\mathrm{train}} $
        \State $ \mathmat{Y}_\mathrm{pred} \gets \Call{predict}{model, node} $ for each $ node \in V \left( G_0 \right) $
        \State $ entropy\_per\_node \gets H \left( \mathmat{Y}_\mathrm{pred} \right) $
        \State \Return $ V \left( G_0 \right) $, sorted in descending order by $ entropy\_per\_node $
    \EndFunction
    \Statex
    \Function{select\_contractions}{$ ordered\_nodes, n_p, \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} $}
        \State $ \mathcal{C}_\mathrm{prolong} \gets \left\{ \right\} $
        \For{$ node \in ordered\_nodes $, \textbf{until} $ \left\lvert \mathcal{C}_\mathrm{prolong} \right\rvert = n_p $}
            \State $ contraction \gets \Call{resolve\_contraction}{node, \mathcal{C}_\mathrm{prolong}, \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)}} $
            \State If $ contraction \neq \mathrm{null} $, add $ contraction $ to $ \mathcal{C}_\mathrm{prolong} $
        \EndFor
        \State \Return $ \mathcal{C}_\mathrm{prolong} $
    \EndFunction
    \Statex
    \Function{resolve\_contraction}{$ node, \mathcal{C}_\mathrm{prolong}, \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} $}
        \State $ contraction \gets \mathrm{null} $
        \For{$ j \in \left\{ 0, \dots, L \right\} $} \Comment I.e. all steps of the original coarsening from finest to coarsest
            \State $ contraction\_candidate \gets $ find in $ \mathcal{C}_j^{(i + 1)} $ a contraction that affects $ node $, if not found, continue with $ j + 1 $
            \If{$ contraction\_candidate \in \mathcal{C}_\mathrm{prolong} $}
                \State \Return $ contraction $
            \EndIf
            \State $ contraction \gets contraction\_candidate $
            \State $ node \gets $ apply $ contraction $ to $ node $, so that in the next loop, a subsequent contraction may be selected
        \EndFor
        \State \Return $ contraction $
    \EndFunction
  \end{algorithmic}
\end{algorithm*}

The procedure keeps track of all the edge contractions that were made in the dataset augmentation part of the algorithm and gradually reverses them. To this end, apart from the embedding \( \Psi_i \), the set of all contractions yet to be reversed as of step \( i \) is kept as \( \mathcal{C}_L^{(i)}, \dots, \mathcal{C}_0^{(i)} \), with the initial values \( \mathcal{C}_j^{(K)} \) corresponding to the underlying coarsening \( \varphi_j \).

In each prolongation step, the embedding \( \Psi_{i + 1} \) is prolonged to \( \Psi_i \) by selecting a set of \( n_p \) contractions \( \mathcal{C}_\mathrm{prolong} \) and undoing them by copying and reusing the embedding of the node resulting from the contraction to both of the contracted nodes. To obtain \( \mathcal{C}_\mathrm{prolong} \), nodes of \( G_0 \) are first ordered in such a way that corresponds to the usefulness of prolonging them. Subsequently, the set \( \mathcal{C}_\mathrm{prolong} \) is selected from \( \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} \) by selecting contractions affecting nodes in the aforementioned order, until \( n_p \) contractions are selected. If multiple contractions affecting the same node are available in the sequence \( \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} \), one is selected from \( \mathcal{C}_j^{(i + 1)} \) corresponding to the coarsest-level coarsening. The sequence \( \mathcal{C}_L^{(i)}, \dots, \mathcal{C}_0^{(i)} \) is produced from \( \mathcal{C}_L^{(i + 1)}, \dots, \mathcal{C}_0^{(i + 1)} \) by removing all of the edges contained in \( \mathcal{C}_\mathrm{prolong} \).

To obtain an ordering of nodes of \( G_0 \) based on the usefulness of their prolongation, the embedding \( \Psi_{i + 1} \) is fully prolonged to a temporary embedding of the full graph, \( \Psi_0^\mathrm{temp} \). The downstream model is then trained using this temporary embedding to obtain \( \mathmat{Y}_\mathrm{pred} \), the predicted posterior distribution of classes for each node in \( G_0 \). The nodes are ordered by the entropy of the posterior, making the algorithm prolong the nodes where the classifier is the most uncertain.
