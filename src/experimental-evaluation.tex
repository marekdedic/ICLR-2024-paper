\section{Experimental evaluation}\label{sec:experimental-evaluation}

The proposed methods were experimentally verified on 10 publicly available datasets. The node2vec algorithm was used for generating the node embeddings, with an MLP classifier providing the predictions for node classification. A detailed description of the used datasets and hyperparameter setup is available in Appendix \ref{sec:app:experiment-details}. In order to study the effect of adaptive prolongation, for each prolongation step, the intermediary embedding was fully prolonged to obtain an embedding of the original graph \( G \). A classifier was then trained with this embedding as input. This setup allows us to compare classification accuracy at each step of the adaptive prolongation, as shown in Figure~\ref{fig:adaptive-coarsening}.

\begin{figure}
  \centering
  \includegraphics[width = 0.65\linewidth]{images/adaptive-coarsening/adaptive-coarsening.pdf}
  \caption{Downstream classifier test-set accuracies at different steps of adaptive prolongation. Dashed line shows the baseline node2vec model accuracy. The node count is taken relative to the total node count in each dataset. The results are averaged over ten runs, with the solid line representing the mean and the shaded area denoting one standard deviation.}
  \label{fig:adaptive-coarsening}
\end{figure}

The results were statistically validated by comparing the adaptive prolongation model to the baseline model at \( k \)-th deciles of node count using the Bayesian Wilcoxon signed-rank test \cite{benavoli_bayesian_2014}. The results of note are that at 60\% complexity, the models have over a 99\% probability of being within 10 percentage points of performance on the full graph and at 80\% complexity, they have over 99\% probability of being withing 5 percentage points of performance.
