\section{An overview of the HARP algorithm}\label{sec:harp}

Our work builds on the HARP method \cite{chen_harp_2018} for pretraining methods such as node2vec \cite{grover_node2vec_2016} on coarsened graphs. The sequence \( G_0, G_1, G_2, \dots, G_L \) is generated in HARP consecutively. In an overview, the HARP algorithm first ahead-of-time consecutively coarsens the graph. The method itself can then be executed by repeating the following steps on the graphs from the coarsest to the finest (i.e., from \( G_L \) to \( G_0 \)):
\begin{enumerate*}
  \item \textbf{Training on an intermediary graph}. The graph embedding model is trained on \( G_i \), producing its embedding \( \Phi_{G_i} \).
  \item \textbf{Embedding prolongation}. The embedding \( \Phi_{G_i} \) is \textit{prolonged} (i.e. refined) into \( \Phi_{G_{i - 1}} \) by copying embeddings of merged nodes. \( \Phi_{G_{i - 1}} \) is then used as the starting point for training on \( G_{i - 1} \).
\end{enumerate*}
