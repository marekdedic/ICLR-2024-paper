# Review 1

## Review

### Summary Of Contributions:

This paper presents a method for graph node classification that can manually select the resolution at which the graph should be simplified.

### Strengths And Weaknesses:

**Strengths:**

1. Graph coarsening is important and practical.
2. The experimental results in Figure 1 seem promising.

**Weaknesses:**

1. The presentation of this paper has large room for improvement. The presentation in its current form makes the reader can not get the main idea easily.
2. The motivation is not clear. Why the author study this problem and why it is important.

### Suggested Changes:

Please address my concerns in the **Weakness** part.

### Rating:

Clear, Correct, and Reproducible (CCR): a submission which meets the reviewing criteria

### Confidence:

3: The reviewer is fairly confident that the evaluation is correct



## Response

1. Re presentation - could you be more concrete?
2. Will improve
















# Review 2

## Review

### Summary Of Contributions:

The paper proposes a novel way of studying relationship between graph coarsening levels and model performance on downstream tasks.

### Strengths And Weaknesses:

**Strengths:**

1. The paper is conscise and clear and can be well understood by readers.
2. The authors propose a method based out of HARP to evaluate the relationship between model performance and graph coarsening levels.

**Weakness:**

1. The utility of this evaluation is not yet clear to me as to obtain graph embeddings a t a level k s.t 0<k<l we still need embeddings of level k+1 till l, hence in no way we are reducing the levels of coarsening that w want to achieve.
2. The paper proposes that by learning on a specific downstream task, it is possible to achieve same performance by reduction of node count by 40%, does that mean that if coarsen a graph by 40% ad then use embedding generated by HARP and predict, we will get same performance as the original graph? This is not yet clear to me

### Suggested Changes:

1. I Would like to get more clarity on utility of this framework and how it will be used.

### Rating:

Great Start (GS): a submission which meets some of the reviewing criteria but has room for improvement

### Confidence:

3: The reviewer is fairly confident that the evaluation is correct


## Response

1. Yes, for an embedding at a level $ k $, we still need embeddings at levels $ k + 1, \dots, L $, but those are simpler ($ L $ is the simplest), and we don't need to train on levels $ k-1, \dots, 0 $, which are the most complex and take the longest to train
2. Essentially, yes - if I coarsen the graph by 40% and predict, I will stay withing 10% of the performance of the original graph - last paragraph of section 3 lists more details about these results.















# Review 3

## Review

### Summary Of Contributions:

This paper addresses the performance/complexity trade-off in the graph node classification task. The authors claim to show that their method provides near-optimal performance at a fraction of the complexity.

### Strengths And Weaknesses:

**Strengths:**

1. The premise of the problem and the description are presented clearly.
2. Figure 2 showing the results consists of a lot of information and they are illustrated in an easy to understand manner.

**Weaknesses:**

1. The abstract could be more descriptive.
2. It is unclear how coarsening and prolongation are exactly decoupled. I apologize if I missed it.

### Suggested Changes:

1. It may be beneficial to provide a quick one sentence definition of coarsening somewhere in the introduction
2. Why do some networks perform significantly poorly compared to others? Any insight into this?

### Rating:

Clear, Correct, and Reproducible (CCR): a submission which meets the reviewing criteria

### Confidence:

3: The reviewer is fairly confident that the evaluation is correct


## Response

**Weaknesses:**

1. Will improve, but limited by page count.
2. In HARP, there are $ L $ coarsening steps producing $ L $ graphs with different coarseness. These are then used (in reverse order) in the prolongation, which therefore also has $ L $ steps. In our method, the prolongation has many more steps $ K $ than the coarsening and produces its own sequence of $ L $ graphs and embeddings (it still uses the original $ K $ graphs from the coarsening stage, but only indirectly). Do you have any suggestion for how to explain this more clearly in the paper?

**Suggestions:**

1. Will improve
2. We have some limited insight that didn't fit the paper, namely we tried to look at the graph homophily, accuracy and assortativity and found them to be highly correlated with performance accross the prolongation procedure. This suggests the explanation of the graphs being heterophilic when very coarse (as could be expected), then reaching a point where the global structure of the graph is in place and is then only refined in a local sense.














# Changes to be made

- [x] Improve abstract
- [x] Simple definition of coarsening in intro?
- [x] Better explanation of sequence complexity
- [x] Better explain why we care about coarsening - motivation behind even studying this
